spring.datasource.url=jdbc:mysql://localhost:3306/seckill?useSSL=false
spring.datasource.username=root
spring.datasource.password=root
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.datasource.hikari.login-timeout=1000
spring.datasource.hikari.maximum-pool-size=30

server.port=8081

mybatis.configuration.map-underscore-to-camel-case=true

#logging.level.com.esb.user=debug
#logging.level.org.springframework.web=debug
#logging.level.org.springframework.transaction=debug
#logging.level.org.mybatis=debug

#logging.config.classpath =log4j2.xml
logging.config=classpath:log4j2.xml

#pagehelper分页插件配置
pagehelper.helperDialect=mysql
pagehelper.reasonable=true
pagehelper.supportMethodsArguments=true
pagehelper.params=count=countSql


# REDIS (RedisProperties)
# Redis数据库索引（默认为0）
spring.redis.database=0
# Redis服务器地址
spring.redis.host=127.0.0.1
# Redis服务器连接端口
spring.redis.port=6379  
# Redis服务器连接密码（默认为空）
spring.redis.password=
# 连接超时时间（毫秒）
spring.redis.timeout=500ms

# 设置缓存默认超过期时间为30秒  
spring.cache.redis.time-to-live.seconds=10

#jedis
# 连接池最大连接数（使用负值表示没有限制）
#spring.redis.jedis.pool.max-active=8  
# 连接池最大阻塞等待时间（使用负值表示没有限制）
#spring.redis.jedis.pool.max-wait=-1
# 连接池中的最大空闲连接
#spring.redis.jedis.pool.max-idle=8  
# 连接池中的最小空闲连接
#spring.redis.jedis.pool.min-idle=0
 
#lettuce客户端   
spring.redis.lettuce.pool.min-idle=0  
spring.redis.lettuce.pool.max-idle=8  
spring.redis.lettuce.pool.max-wait=-1
spring.redis.lettuce.pool.max-active=8  
spring.redis.lettuce.shutdown-timeout=100



#============== kafka ===================
# 指定kafka 代理地址，可以多个
#spring.kafka.bootstrap-servers=192.168.100.10:9092

spring.kafka.bootstrap-servers=192.168.0.107:9092
#=============== provider  =======================

spring.kafka.producer.retries=0
# 每次批量发送消息的数量
spring.kafka.producer.batch-size=16384
spring.kafka.producer.buffer-memory=33554432

# 指定消息key和消息体的编解码方式
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#=============== consumer  =======================
# 指定默认消费者group id
spring.kafka.consumer.group-id=test-consumer-group
#实时生产，实时消费，不会从头开始消费
#earliest
#当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费  
#latest (生产使用)
#当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据 
#none 
# topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=earliest 
#是否自动提交
spring.kafka.consumer.enable-auto-commit=true

#连接超时
kafka.consumer.session.timeout=20000

spring.kafka.consumer.auto-commit-interval=100

#消费线程数
kafka.consumer.concurrency=10

# 指定消息key和消息体的编解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer


# 写入得topic 
kafka.topic.name=topic-seckill
