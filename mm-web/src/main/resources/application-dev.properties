spring.datasource.url=jdbc:mysql://mysql:3306/seckill?useSSL=false
spring.datasource.username=root
spring.datasource.password=root
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.datasource.hikari.login-timeout=1000
spring.datasource.hikari.maximum-pool-size=30

server.port=8082

mybatis.configuration.map-underscore-to-camel-case=true

#logging.level.com.esb.user=debug
#logging.level.org.springframework.web=debug
#logging.level.org.springframework.transaction=debug
#logging.level.org.mybatis=debug

#logging.config.classpath =log4j2.xml
logging.config=classpath:log4j2.xml


#pagehelper分頁插件配置
pagehelper.helperDialect=mysql
pagehelper.reasonable=true
pagehelper.supportMethodsArguments=true
pagehelper.params=count=countSql



# REDIS (RedisProperties)
# Redis資料庫索引（預設為0）
spring.redis.database=0
# Redis服務器地址
spring.redis.host=redis
# Redis服務器連接端口
spring.redis.port=6379  
# Redis服務器連接密碼（預設為空）
spring.redis.password=
# 連接超時時間（毫秒）
spring.redis.timeout=500ms

# 設置緩存預設超過期時間為30秒
spring.cache.redis.time-to-live.seconds=10
 
#lettuce客户端   
spring.redis.lettuce.pool.min-idle=0  
spring.redis.lettuce.pool.max-idle=8  
spring.redis.lettuce.pool.max-wait=-1
spring.redis.lettuce.pool.max-active=8  
spring.redis.lettuce.shutdown-timeout=100



#============== kafka ===================
# 指定kafka 代理地址，可以多個
#spring.kafka.bootstrap-servers=192.168.100.10:9092

spring.kafka.bootstrap-servers=kafka01:9092,kafka02:9093
#=============== provider  =======================

spring.kafka.producer.retries=0
# 毎次批次及送消息的數量
spring.kafka.producer.batch-size=16384
spring.kafka.producer.buffer-memory=33554432

# 指定消息key和消息本身的編解碼方式
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#=============== consumer  =======================
# 指定預設消費者group id
spring.kafka.consumer.group-id=test-consumer-group
#即時生產，即時消費，不會從頭開始消費
#earliest
#當各分區下有己提交的offset時，從提交的offset開始消費；無提交的offset時，從頭開始消費  
#latest (生產使用)
#當各分區下有己提交的offset時，從提交的offset開始消費；無提交的offset時，消費新產生的該分區下的數據 
#none 
# topic各分區都存在己提交的offset時，從offset後開始消費；只要有一個分區不存在己提交的offset，則抛出異常
spring.kafka.consumer.auto-offset-reset=earliest 
#是否自動提交
spring.kafka.consumer.enable-auto-commit=true

#連接超時
kafka.consumer.session.timeout=20000

spring.kafka.consumer.auto-commit-interval=100

#消費線程數
kafka.consumer.concurrency=10

# 指定消息key和消息体的编解码方式
# 指定消自key和消息本身的編解碼方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# 寫入的topic 
spring.kafka.topic.name=topic-seckill
